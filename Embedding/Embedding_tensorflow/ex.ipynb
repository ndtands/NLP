{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "tf.keras.layers.Embedding(\n",
    "    input_dim, output_dim, embeddings_initializer='uniform',\n",
    "    embeddings_regularizer=None, activity_regularizer=None,\n",
    "    embeddings_constraint=None, mask_zero=False, input_length=None, **kwargs\n",
    ")\n",
    "```\n",
    "- Input_dim: số lượng vocab tối đa cần được ước lượng\n",
    "- output_dim: kích thước embedding mong muốn\n",
    "- input_length: số lượng maxlen của câu đầu vào.\n",
    "\n",
    "\n",
    "## Cách hoạt động:\n",
    "\n",
    "1 câu (max_len x 1) => One hot encoding (max_len x Vocab_size) => hiddenLayer E (Vocab_size x N) => output (max_len x N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import some lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imdb.vocab', 'imdbEr.txt', 'README', 'test', 'train']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "# dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
    "#                                   untar=True, cache_dir='.',\n",
    "#                                   cache_subdir='')\n",
    "\n",
    "# dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "os.listdir('aclImdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labeledBow.feat',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'unsup',\n",
       " 'unsupBow.feat',\n",
       " 'urls_neg.txt',\n",
       " 'urls_pos.txt',\n",
       " 'urls_unsup.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = os.path.join('aclImdb', 'train')\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_dir = os.path.join(train_dir, 'unsup')\n",
    "# shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75000 files belonging to 3 classes.\n",
      "Using 60000 files for training.\n",
      "Found 75000 files belonging to 3 classes.\n",
      "Using 15000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "seed = 123\n",
    "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=batch_size, validation_split=0.2,\n",
    "    subset='training', seed=seed)\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=batch_size, validation_split=0.2,\n",
    "    subset='validation', seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 b\"Ask yourself where she got the gun? Remember what she was taught about the mark's mindset when the con is over? The gun had blanks and it was provided to her from the very beginning.<br /><br />When the patient comes back at the end she was SUPPOSED to see him drive away in the red convertible and lead her to the gang splitting up her 80 thousand.<br /><br />The patient was in on the con from the beginning.<br /><br />Mantegna does not die in the end - the gun had blanks.<br /><br />There - enough spoilers for you there? This is why people are giving it such high ratings. It's extremely original because of the hidden ending and how it cons MOST of the audience.\"\n",
      "2 b\"For some reason, people seem to have a problem differentiating this movie from Trail of the Pink Panther.<br /><br />At any rate, this work does nothing but serve to remind us how sad the world is without Peter Sellers in it.<br /><br />They brought back the same old favorites from Trail (Dreyfus, Cato, Litton, etc.), but they introduced a misdirected Pratt-fall humorist into a role which was designed to substitute for the missing Clouseau. <br /><br />Dreyfus devises a way to produce the perfect copy of Clouseau via a hard frame computer system which factors the variables and tosses out the name of the most inept idiot in the global law enforcement family. What we got was a poor guy who was obviously overwhelmed by the grand scale of what Blake Edwards proposed he should do, and boy does it show.<br /><br />Ted Wass was amiable as Sergeant Clifton Sleigh, but let's face it...he wasn't Clouseau in any way. I realize that Blake Edwards was losing his greatest cash cow, but to disrespect Sellers' memory like this was just sacrilege. Frankly, I'm glad they have remade the original. I hope it runs a long line of successful ventures for Steve Martin.<br /><br />This dreck rates a 2.0/10 from...<br /><br />the Fiend :.\"\n",
      "2 b\"Jimeoin is a nameless actor who finds himself as the eternal extra\\xc2\\x85 never to play a principle role in a movie. He finds himself caught up with a group of would-be stars all trying to gain a break but none of them are able to do so.<br /><br />I was ready for a good comedy, but was bitterly disappointed. Jimeoin is a great comedian but this smacked of 'try hard' and it just failed. There were a few moments where I laughed out aloud and I recognised several moments of clever humour, but it wasn't enough. I enjoyed spotting the good familiar Aussie actors and scenes around Melbourne. I think you should spend your money seeing something else...\"\n",
      "2 b'\"And I\\'ve had vould have gotten avay vith it, too, if it veren\\'t for you meddling Ritzes! Blah, blah!\" <br /><br />No, not really. Poor Bela was continuing his spiraling descent from the triumph of Dracula to working for Ed Wood. He actually has a half comedic, half heroic role in this movie, but mostly he spends the movie being a distraction from what could laughably be called a plot.<br /><br />One has to wonder if the Scooby-Doo cartoons were inspired by movies like this. YOu had everything you see in your average Scooby cartoon- secret passages, some guy in a costume, ulterior motives (which, unlike a Scooby cartoon, don\\'t actually make sense here.) <br /><br />Okay, the Ritz Brothers. They were a very popular vaudeville act back in the day, but no one remembers them much today. Watching this film, you can see why. They didn\\'t have the comic timing, distinct personalities or perfect slapstick of the Three Stooges or Marx Brothers. They were pretty much interchangeable, with Jimmy and Al mugging while Harry got most of the dialog.<br /><br />There is a bit of interesting Hollywood history that the Ritzes staged a \"walkout\" on this film, to protest the quality of the script. 20th Century Fox should have let them walk and reworked the script. Instead, they finished the movie, and Fox kicked them to the curb.<br /><br />I don\\'t understand the plot. I guess that Atwill was supposed to be the villain, but really the guy who was pretending to be SEC agent was the actual killer, but it was never clear why he was killing people or why he would walk into the trap that Atwill and Lugosi had set for him. The ending makes absolutely no sense.<br /><br />You almost get the impression that there were a lot of b-listers (The Ritzes, Lugosi, Atwill, Patsy Kelly) who were insisting that THEY get more screen time than the others. Other characters, like the \"seaman\" who is found in the closet, are introduced and no explanation is given as to what they were doing there.'\n",
      "1 b'I think if you were to ask most JW\\'s whether they expect a miracle cure because of their faith, you will find they do not. I know I do not. What you will find instead is that they believe the promises Christ made of a resurrection. So, even even if the worst were to happen and we die while holding onto our integrity, Jehovah can, and will correct this.<br /><br />It really gets down to a simple question: is God real to you or is this all just make believe? If he is real, and you trust him, you will follow his directions no matter what the short term outcome may be.<br /><br />I had a heart attack about a year and a half ago. One in my family was horrified when she saw the words \"NO BLOOD\" written in large letters over my chart. I reasoned with her that if I were in a position that only a blood transfusion would save my life, would that be a good time to anger the only one could return me to life when the time came? She didn\\'t get it -- God just isn\\'t real enough to her. Too bad. I wish she could have the comfort a strong faith gives.'\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(5):\n",
    "    print(label_batch[i].numpy(), text_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed a 1,000 word vocabulary into 5 dimensions.\n",
    "embedding_layer = tf.keras.layers.Embedding(1000, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04159603,  0.01817093,  0.03314178, -0.00271792,  0.00815636],\n",
       "       [-0.03416543,  0.0154368 ,  0.00639908,  0.02999461, -0.04190104],\n",
       "       [ 0.00738442, -0.02693652,  0.00236541, -0.02436489,  0.04716805]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([1, 2, 3]))\n",
    "result.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([[0, 1, 2], [3, 4, 5]]))\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom standardization function to strip HTML break tags '<br />\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />',' ')\n",
    "    return tf.strings.regex_replace(stripped_html, '[%s]' % re.escape(string.punctuation), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary size and number of words in a sequence.\n",
    "vocab_size = 10000\n",
    "sequence_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Note that the layer uses the custom standardization defined above.\n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "model = Sequential([\n",
    "  vectorize_layer,\n",
    "  Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
    "  GlobalAveragePooling1D(),\n",
    "  Dense(16, activation='relu'),\n",
    "  Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "59/59 [==============================] - 29s 462ms/step - loss: 0.5180 - accuracy: 0.1656 - val_loss: 0.2648 - val_accuracy: 0.1711\n",
      "Epoch 2/15\n",
      "59/59 [==============================] - 6s 104ms/step - loss: -0.1966 - accuracy: 0.1655 - val_loss: -0.7572 - val_accuracy: 0.1690\n",
      "Epoch 3/15\n",
      "59/59 [==============================] - 6s 108ms/step - loss: -1.5155 - accuracy: 0.1661 - val_loss: -2.3472 - val_accuracy: 0.1690\n",
      "Epoch 4/15\n",
      "59/59 [==============================] - 6s 103ms/step - loss: -3.6823 - accuracy: 0.1661 - val_loss: -5.1874 - val_accuracy: 0.1690\n",
      "Epoch 5/15\n",
      "59/59 [==============================] - 6s 101ms/step - loss: -7.6414 - accuracy: 0.1661 - val_loss: -10.2334 - val_accuracy: 0.1690\n",
      "Epoch 6/15\n",
      "59/59 [==============================] - 6s 98ms/step - loss: -14.2341 - accuracy: 0.1661 - val_loss: -18.1863 - val_accuracy: 0.1690\n",
      "Epoch 7/15\n",
      "59/59 [==============================] - 6s 99ms/step - loss: -24.1263 - accuracy: 0.1661 - val_loss: -29.6499 - val_accuracy: 0.1690\n",
      "Epoch 8/15\n",
      "59/59 [==============================] - 6s 103ms/step - loss: -37.9072 - accuracy: 0.1661 - val_loss: -45.1666 - val_accuracy: 0.1690\n",
      "Epoch 9/15\n",
      "59/59 [==============================] - 6s 99ms/step - loss: -56.1106 - accuracy: 0.1661 - val_loss: -65.2316 - val_accuracy: 0.1690\n",
      "Epoch 10/15\n",
      "59/59 [==============================] - 6s 100ms/step - loss: -79.2254 - accuracy: 0.1661 - val_loss: -90.3004 - val_accuracy: 0.1690\n",
      "Epoch 11/15\n",
      "59/59 [==============================] - 6s 101ms/step - loss: -107.7036 - accuracy: 0.1661 - val_loss: -120.7954 - val_accuracy: 0.1690\n",
      "Epoch 12/15\n",
      "59/59 [==============================] - 6s 101ms/step - loss: -141.9648 - accuracy: 0.1661 - val_loss: -157.1095 - val_accuracy: 0.1690\n",
      "Epoch 13/15\n",
      "59/59 [==============================] - 6s 107ms/step - loss: -182.3999 - accuracy: 0.1661 - val_loss: -199.6090 - val_accuracy: 0.1690\n",
      "Epoch 14/15\n",
      "59/59 [==============================] - 6s 104ms/step - loss: -229.3735 - accuracy: 0.1661 - val_loss: -248.6362 - val_accuracy: 0.1690\n",
      "Epoch 15/15\n",
      "59/59 [==============================] - 6s 102ms/step - loss: -283.2267 - accuracy: 0.1661 - val_loss: -304.5114 - val_accuracy: 0.1690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f45f13d4c0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, 100)              0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 100, 16)           160000    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 16)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                272       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 15996."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#docs_infra: no_execute\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the trained word embeddings and save them to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using this 2 file to http://projector.tensorflow.org/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"eda.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dimension = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "684b1123683431d89d3bfe9a89cc763215f4b8cd94b4aba1fb40ad45ff7c8b41"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
